#å‰å‘å‚³æ’­æ­å»ºç¶²çµ¡çµæ§‹
#å¾Œå‘å‚³æ’­è¨“ç·´ç¶²çµ¡åƒæ•¸

#æ¿€æ´»å‡½æ•¸æœ‰ä¸‰ç¨®
# 1.relu
# 2.sigmoid
# 3.tanh

#NNè¤‡é›œåº¦:å¤šç”¨NNå±¤æ•¸å’ŒNNåƒæ•¸çš„å€‹æ•¸è¡¨ç¤º
#å±¤æ•¸=éš±è—å±¤çš„å±¤æ•¸ + 1å€‹è¼¸å‡ºå±¤
##################################################

#æå¤±å‡½æ•¸(loss):é æ¸¬å€¼ èˆ‡ å·²çŸ¥ç­”æ¡ˆçš„å·®è·
#NNå„ªåŒ–ç›®æ¨™ç‚ºlossæœ€å°
#ä¸»æµçš„lossè¨ˆç®—æ–¹æ³•æœ‰ä¸‰ç¨®å¦‚ä¸‹
#1.å‡æ–¹èª¤å·®
#2.è‡ªå®šç¾©
#3.äº¤å‰ç†µ

#åœ¨æ‰€æœ‰åƒæ•¸ä¸Šç”¨æ¢¯åº¦ä¸‹é™çš„æ–¹æ³•,ä½¿è¨“ç·´é›†æ•¸æ“šä¸Šçš„æå¤±å‡½æ•¸æœ€å°
# æå¤±å‡½æ•¸(loss):é æ¸¬(y) èˆ‡ å·²çŸ¥ç­”æ¡ˆ(y_) çš„å·®è·
# æ–¹æ³•ä¸€ å‡æ–¹èª¤å·® loss=tf.reduce_mean(tf.square(y-y_))

# æ–¹æ³•äºŒ åå‘å‚³æ’­è¨“ç·´: ä»¥æ¸›å°‘losså€¼å¨å„ªåŒ–ç›®æ¨™  æœ‰ä¸‰ç¨®å¯é¸å¦‚ä¸‹ğŸ‘‡ è¨“ç·´é¸æ“‡å…¶ä¸€å³å¯
# 1. train_step=tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)
# 2. train_step=tf.train.MomentumOptimizer(learning_rate,momentum).minimize(loss)
# 3. train_step=tf.train.AdamOptimizer(learning_rate,momentum).minimize(loss)
#éƒ½æœ‰ä¸€å€‹learning_rate(å­¸ç¿’ç‡:æ±ºå®šåƒæ•¸æ¯æ¬¡æ›´æ–°çš„å¹…åº¦)çš„åƒæ•¸

import os
os.environ["TF_CPP_MIN_LOG_LEVEL"]="2"
import tensorflow as tf
import numpy as np
# é—œé–‰ç·Šæ€¥åŸ·è¡Œ
tf.compat.v1.disable_eager_execution()
BATCH_SIZE=8 #ä¸€æ¬¡é¤µå…¥ç¥ç¶“ç¶²çµ¡8çµ„æ•¸æ“š
SEED=23455

#åŸºæ–¼seedç”¢ç”Ÿéš¨æ©Ÿæ•¸
rdm=np.random.RandomState(SEED)#ä¿è­‰æ¯æ¬¡éš¨æ©Ÿç”Ÿå­˜æˆçš„æ•¸å­—ä¸€æ¨£

#éš¨æ©Ÿæ•¸è¿”å›32è¡Œ2åˆ—çš„çŸ©é™£ è¡¨ç¤º32çµ„ 2å€‹ç‰¹å¾(é«”ç©å’Œé‡é‡) ä½œç‚ºè¼¸å…¥æ•¸æ“šé›†
X=rdm.rand(32,2)
Y=[[x1+x2+(rdm.rand()/10.0-0.05)] for (x1,x2) in X]

#å®šç¾©ç¥ç¶“ç¶²çµ¡çš„è¼¸å…¥,åƒæ•¸å’Œè¼¸å‡º,å®šç¾©å‰å‘å‚³æ’­éç¨‹
x=tf.compat.v1.placeholder(tf.float32,shape=(None,2)) #è¼¸å…¥æ•¸æ“šä½”ä½
y_=tf.compat.v1.placeholder(tf.float32,shape=(None,1)) #æ¨™æº–ç­”æ¡ˆæ•¸æ“šä½”ä½
w1=tf.Variable(tf.compat.v1.random_normal(shape=(2,1),stddev=1,seed=1)) #2å€‹ç‰¹å¾,1å€‹è¼¸å…¥
y=tf.matmul(x,w1) #ä¹˜æ³•çŸ©é™£

#å®šç¾©æå¤±å‡½æ•¸ç‚ºMSE,åå‘å‚³æ’­æ–¹æ³•ç‚ºæ¢¯åº¦ä¸‹é™
loss_mse=tf.reduce_mean(tf.square(y_-y))#å‡æ–¹èª¤å·®è¨ˆç®—loss
train_step=tf.compat.v1.train.GradientDescentOptimizer(0.001).minimize(loss_mse) #å­¸ç¿’ç‡0.001
# train_step=tf.compat.v1.train.MomentumOptimizer(0.001,0.9).minimize(loss)
# train_step=tf.compat.v1.train.AdamOptimizer(0.001).minimize(loss)

#ç”Ÿå­˜æœƒè©±,è¨“ç·´STEPSè¼ª
with tf.compat.v1.Session() as sess:
    #åˆå§‹åŒ–
    init_op=tf.compat.v1.global_variables_initializer()
    sess.run(init_op)

    #è¨“ç·´æ¨¡å‹
    STEPS=20000
    for i in range(STEPS):
        strat=(i*BATCH_SIZE) % 32 #å› ç‚ºåªæœ‰32çµ„æ•¸æ“šæ‰€ä»¥%32
        end=(i*BATCH_SIZE) % 32 +BATCH_SIZE
        sess.run(train_step,feed_dict={x:X[strat:end],y_:Y[strat:end]})
        if i % 500  == 0:
            print("ç¬¬{0}æ¬¡è¨“ç·´".format(i))
            print("w1ç‚º:",sess.run(w1))
    print("æœ€å¾Œçš„çµæœç‚º:",sess.run(w1))