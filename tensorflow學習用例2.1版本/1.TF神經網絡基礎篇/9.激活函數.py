import tensorflow as tf

# Sigmoid函數
# tf.nn.sigmoid(x)    #f(x) = 1 / (1+(e**(-x)))
# 特點:
# 1.易造成梯度消失
# 2.輸出非0均值,收斂慢
# 3.冪運算複雜,訓練時間長


# Tanh函數
# tf.math.tanh(x)    #f(x) = (1-(e**(-2x))) / (1+(e**(-2x)))
# 特點:
# 1.輸出是0均值
# 2.易造成梯度消失
# 3.冪運算複雜,訓練時間長


# Relu函數                                  { 0, x<0   當x小於0時,輸出0
# tf.nn.relu(x)     #f(x) = max(x,0)     =  { x, x>=0 當x大於0時,輸出x
# 優點
# 1.解決了梯度消失問題(在正區間)
# 2.只需判斷輸入是否大於0,計算速度快
# 3.收斂速度遠快於 sigmoid 和 tanh
# 缺點:
# 1.輸出非0均值,收斂慢
# 2.Dead ReIU問題:某神經元可能永遠不會被激活,導致相應的參數永遠不能被更新


# Leaky Relu函數
# tf.nn.leaky_relu(x)   #f(x) = max(ax,x)  #引入斜率a,使負區間不再恆等於0
# 理論上來講, Leaky Relu有Relu的所有優點,外加不會有Dead Relu問題,但是在實際操作當中,並沒有完全證明Leaky Relu總是好於Relu



# 對於初學者的建議:
# 1.首選relu激活函數
# 2.學習率設置較小值
# 3.輸入特征標準化,即讓輸入特征滿足以0為均值,1為標準差的正態分佈
# 4.初始參數中心化,即讓隨機生成的參數滿足以0為均值, ((2/當前層輸入特征個數)開根號)為標準差的正態分佈