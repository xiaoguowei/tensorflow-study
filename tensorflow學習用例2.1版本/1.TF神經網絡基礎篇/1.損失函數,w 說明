1.損失函數(loss):表征了網絡前向傳播的結果預測值(y)與正確答案(y_)之間的差距,損失函數可以定量判斷W,b的優劣,當損失函數輸出最小時,參數W,b會出現最優值,
              我們的目的是要尋找一組參數W和b使得損失函數最小

2.梯度下降法:沿著損失函數梯度下降的方向,尋找損失函數的最小值,得到最優參數的方法

3.學習率(learning rate→ 簡稱 lr):當學習率設置的過小時,收斂過程將變得十分緩慢.
                               而當學習率設置的過大時,梯度可能會在最小值附近來回震蕩,甚至可能無法收斂

4.反向傳播:從後向前,逐層求損失函數對每層神經元參數的偏執數,迭代更新所有參數

5.參數w迭代過程:        Wt+1=Wt - lr * (aloss/aWt)👇
                    👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇
                    loss=(w + 1)的平方    (aloss/aW)=(2 * W) + 2
                    👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇👇
                    參數初始化為5,學習率為0.2,則:
                    1次  參數W:5      5-0.2 * (2 * 5 + 2) = 2.6
                    2次  參數W:2.6    2.6-0.2 * (2 * 2.6 + 2 ) = 1.16
                    3次  參數W:1.16   1.16-0.2 * (2 * 1.16 + 2 ) = 0.296
                    4次  參數W:0.296
                    最終目的:找到loss最小 既 W = -1 的最優參數W